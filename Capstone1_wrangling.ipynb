{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Capstone 1 Wrangling\n",
    "\n",
    "All companies that participate in wholesale energy markets are required to submit a quarterly report detailing each transaction to the Federal Energy Regulatory Commission (FERC). This information is then made publicly available for download on FERCâ€™s website at https://eqrreportviewer.ferc.gov/. The website allows you to download a zipped file that contains all of the submissions made for each quarter. Each submission is kept in a zipped folder and can contain up to 4 csv files:\n",
    "\n",
    "1.\tIdentity table. Required to be submitted by all entities. Contains information about the entity that is filing the report.\n",
    "2.\tIndex table. Required to be submitted by all entities, but can be left blank if nothing to be reported. Lists the index price publishers to which transactions have been reported.\n",
    "3.\tContracts table. Only filed if the entity has something to report. Lists any all of the currently active wholesale contracts where the entity is the seller of energy.\n",
    "4.\tTransactions table. Only filed if the entity has something to report. Lists all of the sales of energy made by the entity.\n",
    "\n",
    "Once downloaded the first step is to unzip each quarterly filing and each individual submission folder. Since thousands of folders needed to be unzipped I loaded all of the zipped quarterly filings into a single directory and then used the below to programmatically unzip and move each one to a new directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import zipfile\n",
    "import os \n",
    "import fnmatch as fn\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "#start = time.time()\n",
    "\n",
    "root = r\"C:\\Users\\anhem44\\Desktop\\Capstone1\\Zip\"\n",
    "pattern = '*.zip'\n",
    "\n",
    "for root, dirs, files in os.walk(rootPath):\n",
    "    for filename in fnmatch.filter(files, pattern):\n",
    "        zipfile.ZipFile(os.path.join(root, filename)).extractall(os.path.join(r\"C:\\Users\\anhem44\\Desktop\\Capstone1\\unzip1\", os.path.splitext(filename)[0]))\n",
    "\n",
    "root = r\"C:\\Users\\anhem44\\Desktop\\Capstone1\\unzip1\"\n",
    "\n",
    "for root, dirs, files in os.walk(rootPath):\n",
    "    for filename in fnmatch.filter(files, pattern):\n",
    "        zipfile.ZipFile(os.path.join(root, filename)).extractall(os.path.join(r\"C:\\Users\\anhem44\\Desktop\\Capstone1\\unzip2\", os.path.splitext(filename)[0]))\\\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "At this point I was left with a directory filled with thousands of unzipped folders, each of which contained between 2 and 4 .csv files. For the purposes of my project I only need the transactions table (#4 above) so I wrote a the belpw scropt which looped through each folder, pulled out only the transactions table (this was straightforward as entities are required to submit these files as entityname_transactions.csv), and then concatenated each .csv file to a pandas data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root = r\"C:\\Users\\anhem44\\Desktop\\Capstone1\\unzip2\"\n",
    "pattern = \"*transactions.csv\"\n",
    "combined_csv = pd.DataFrame()\n",
    "\n",
    "dtypes = {'transaction_unique_id':'str',\n",
    " 'seller_company_name':'str',\n",
    " 'customer_company_name':'str',\n",
    " 'ferc_tariff_reference':'str',\n",
    " 'contract_service_agreement':'str',\n",
    " 'transaction_unique_identifier':'str',\n",
    " 'transaction_begin_date':'str',\n",
    " 'transaction_end_date':'str',\n",
    " 'trade_date':'str',\n",
    " 'exchange_brokerage_service':'str',\n",
    " 'type_of_rate':'str',\n",
    " 'time_zone':'str',\n",
    " 'point_of_delivery_balancing_authority':'str',\n",
    " 'point_of_delivery_specific_location':'str',\n",
    " 'class_name':'str',\n",
    " 'term_name':'str',\n",
    " 'increment_name':'str',\n",
    " 'increment_peaking_name':'str',\n",
    " 'product_name':'str',\n",
    " 'transaction_quantity':'float',\n",
    " 'price':'float',\n",
    " 'rate_units':'str',\n",
    " 'standardized_quantity':'float',\n",
    " 'standardized_price':'float',\n",
    " 'total_transmission_charge':'float',\n",
    " 'total_transaction_charge':'float'}\n",
    "\n",
    "for path, subdirs, files in os.walk(root):\n",
    "    filenames = [] \n",
    "    for name in files:\n",
    "        try :\n",
    "            if fn.fnmatch(name, pattern):\n",
    "                filenames.append(os.path.join(path, name))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "for f in filenames :\n",
    "    try:\n",
    "        combined_csv = pd.concat([combined_csv, pd.read_csv(f,dtype=dtypes,parse_dates=[\"trade_date\"])])\n",
    "    except:\n",
    "        pass\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the transactions table transaction begin date and transaction end date were stored as integers (e.g. 20160716120000 for 2016-07-16 12:00:00) so I used the following to convert these integers into the appropriate date time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_csv['transaction_begin_date'] = pd.to_datetime(combined_csv['transaction_begin_date'])      \n",
    "combined_csv['transaction_end_date'] = pd.to_datetime(combined_csv['transaction_end_date'])  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, since the goal of this project was to predict prices for a FERC specific time period and season I used the following to create these variables in the data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_csv['FERC_time'] = 'off_peak'\n",
    "combined_csv['FERC_time'][(combined_csv['transaction_begin_date'].dt.weekday <= 5) & (combined_csv['transaction_begin_date'].dt.hour >= 6) & (combined_csv['transaction_begin_date'].dt.hour <= 21)]='peak' \n",
    "\n",
    "combined_csv['FERC_season'] = 'shoulder'\n",
    "combined_csv['FERC_season'][(combined_csv['transaction_begin_date'].dt.month <= 2) | (combined_csv['transaction_begin_date'].dt.month >= 12)]='winter' \n",
    "combined_csv['FERC_season'][(combined_csv['transaction_begin_date'].dt.month <= 8) & (combined_csv['transaction_begin_date'].dt.month >= 6)]= 'summer'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicities sake, I've exported this data frame to a csv so that I don't have to clean the data every time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combined_csv.to_csv(r\"C:\\Users\\anhem44\\Desktop\\Test\\combined_csv.csv\")\n",
    "#end = time.time()\n",
    "#print(end - start)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
